\section{Kernels}

\subsection{Chapter 2, 3(August 20)}

\subsubsection*{Discussions}
\begin{itemize}
\item What is Mercer's theorem?
\end{itemize}

\subsubsection*{Topics for Review}
\begin{itemize}
\item Dual representation
\item Ridge regression
\item Kernel trick, kernel function
\item PCA
\item Mercer's theorem
\item covariance kernels
\item Operations that preserve kernels
\end{itemize}

\subsection{Chapter 5, 6, 7 (August 22)}

\subsubsection*{Discussions}
\begin{itemize}
\item operation on Gram matrix (e.g. centering)
\item gram schmidt -- project and normalize
\item gram schimdt in feature space -- cholesky on gram matrix
\item fischer vs LDA
\item CCA - primal
\item SVMs
\end{itemize}

\subsubsection*{Topics for Review}
\begin{itemize}
\item Know basics on Gram matrices
\item Know Fischer Objective + Kernlization
\item Covariance - Kernel -- eigenvector/eigenvalue relationship
\item Kernlized PCA
\item CCA
\item SVM Derivation
\item Perceptron Convergence
\end{itemize}
