\section{Natural Language Processing and Speech}

\subsection{Chapter 1-4 (July 18)}

\subsubsection*{Discussions}
\begin{itemize}
\item Are n-grams defined over word forms or word roots ? Ans - application specific.
\item Good-Turing smoothing : we derived Eq 4.27 using Eq 4.26 and briefly discussed the approximation used for larger N (Simple-Good turing)
\item Brief discussion of interpolation and katz-backoff.
\item Discussed back-off in Kneser Ney. An unanswered question was regarding implementation of back-off from n-grams to (n-1)-grams for $n > 1$ (do we use context or back-off to Kneser-Ney probabilities ?)
\end{itemize}

\subsubsection*{Topics for Review}
\begin{itemize}
\item Kneser-Ney
\item Perplexity
\item Good Turing (formula)
\end{itemize}

\subsection{Chapter 5-9 (July 22)}

\subsubsection*{Discussions}
\begin{itemize}
\item Maximum entropy Markov model (how this differs from hidden Markov model)
\item Hidden Markov model speech recognition (how the model works)
\end{itemize}

\subsubsection*{Topics for Review}
\begin{itemize}
\item Part of speech tagging (hidden Markov models and rule based)
\item Maximum entropy and maximum entropy markov models
\item features for speech (pitch, amplitude, spectrograms, formants)
\item Cepstrum
\item Speech recognition with hidden Markov model
\end{itemize}

\subsection{Chapter 10, 12-13 (July 25)}
\subsubsection*{Discussions}
\begin{itemize}
\item Viterbi Approximation
\item Only simple language models
\item multi-pass -- do coarse solution proposal, exact comparison of proposals
\item word lattices -- not 100\% on how to build it, but its probably just a modified viterbi
\item confidence
\item A* decoding -- fast match; g(cost) + partial path is confusing
\item triphones -- bigram to trigram model; state space expansion; decision tree
\item MMIE -- wtf
\item adaptation -- gender; Maximum Likelihood Linear Regression (MLLR)
\item Dependency vs CFG
\item Bottom-up vs top-down
\item parsing hard b/c ambiguity
\item CKY 
\end{itemize}
\subsubsection*{Topics for Review}
\begin{itemize}
\item Stack/ A* decoding
\item Multi-Pass
\item Tri-Phones
\item Adaptation
\item Know Context Free Grammars
\item Chomsky Normal Form
\end{itemize}

\subsection{Chapter 14-15, 17 (July 29)}
\subsection*{Discussions}
\begin{itemize}
\item Inside-outside algorithm relation to EM? (It seems to be closer to forward-backward)
\end{itemize}

\subsection*{Topics for Review}
\begin{itemize}
\item PCFGs, probabilistic CKY, inside-outside algorithm
\item Mitigating poor independence assumptions: splitting tags, split and merge
\item Mitigating lack of lexical conditioning: head word tagging, Collins parser
\item Feature structures - used to prevent an explosion of grammar rules, augmented Earley parser
\end{itemize}
