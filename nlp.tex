\section{Natural Language Processing and Speech}

\subsection{Chapter 1-4 (July 18)}

\subsubsection*{Discussions}
\begin{itemize}
\item Are n-grams defined over word forms or word roots ? Ans - application specific.
\item Good-Turing smoothing : we derived Eq 4.27 using Eq 4.26 and briefly discussed the approximation used for larger N (Simple-Good turing)
\item Brief discussion of interpolation and katz-backoff.
\item Discussed back-off in Kneser Ney. An unanswered question was regarding implementation of back-off from n-grams to (n-1)-grams for $n > 1$ (do we use context or back-off to Kneser-Ney probabilities ?)
\end{itemize}

\subsubsection*{Topics for Review}
\begin{itemize}
\item Kneser-Ney
\item Perplexity
\item Good Turing (formula)
\end{itemize}

\subsection{Chapter 5-9 (July 22)}

\subsubsection*{Discussions}
\begin{itemize}
\item Maximum entropy Markov model (how this differs from hidden Markov model)
\item Hidden Markov model speech recognition (how the model works)
\end{itemize}

\subsubsection*{Topics for Review}
\begin{itemize}
\item Part of speech tagging (hidden Markov models and rule based)
\item Maximum entropy and maximum entropy markov models
\item features for speech (pitch, amplitude, spectrograms, formants)
\item Cepstrum
\item Speech recognition with hidden Markov model
\end{itemize}
