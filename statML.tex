\section{Machine Learning}

\subsection{Chapter 2-7, 17 (August 12)}

\subsubsection*{Discussions}
\begin{itemize}
\item Rundown of sum-product
\item Junction tree
\end{itemize}

\subsubsection*{Topics for Review}
\begin{itemize}
\item Independence and graphs
\item Elimination - sum over all factors a variables appears in
\item Reconstituted graph
\item Moralization - resulting graph might have fewer independence assumptions
\item Complexity of elimination is exponential in the size of the elimination clique
\item Tree-structured elimination - sum-product/message passing
\item Junction tree
\end{itemize}


\subsection{Chapter 5-7, 10-12 (August 15)}

\subsubsection*{Discussions}
\begin{itemize}
\item Bayesians vs Frequentists overview
\item LMS algorithm
\item Generative models 
\item Iteratively reweighted least squares
\item Brief overview of other classification methods
\item EM : the incomplete likelihood vs expected conditional likelihood
\item KL divergence perspective of EM
\end{itemize}

\subsubsection*{Topics for Review}
\begin{itemize}
\item IRLS (and overview of other classification algorithms)
\item EM
\item Practice EM by derivations of conditional mixtures etc
\end{itemize}