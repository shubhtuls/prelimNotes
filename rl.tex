\section{Reinforcement Learning}

\subsection{Chapter 1-5 (August 5)}

\subsubsection*{Discussions}
\begin{itemize}
\item No discussion
\end{itemize}

\subsubsection*{Topics for Review}
\begin{itemize}
\item n-armed bandit
\item epsion-greedy algorithms, softmax action selection
\item MDPs, Bellman equations
\item value iteration, policy iteration, do things asynchronously
\item Monte Carlo policy evaluation
\end{itemize}

\subsection{Chapter 6, 8, 11 (August 8)}

\subsubsection*{Discussions}
\begin{itemize}
\item n-armed bandits
\item pursuit
\item off-policy monte carlo control
\item TD
\item Sarsa
\item Q-learning
\item R-Learning -- vis-a-vis ergodicity
\item Ch. 7 brief
\item TD($\lambda$)-like updates
\end{itemize}

\subsubsection*{Topics for Review}
\begin{itemize}
\item pursuit, exploration methods
\item on-policy/off-policy difference
\item all-visits vs first visit
\item 134/fig 6.4
\item 6.3 -- optimality of TD(0)
\item Sarsa + Q-learning (TD + control, off-policy vs on-policy)
\item fig 6.15/6.16
\item Ch.7 figs
\item 8.1
\end{itemize}
